{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # loads .env into os.environ\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# print(groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq is compatible with the same client\n",
    "client = OpenAI(\n",
    "\tapi_key=groq_api_key, \n",
    "\tbase_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\tmodel=\"meta-llama/llama-4-scout-17b-16e-instruct\", # for other models, check https://console.groq.com/docs/models\n",
    "\tmessages=[\n",
    "\t\t{\"role\": \"user\", \"content\": \"Explain what is RAG\"}\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval-Augmented Generator. It's a type of natural language processing (NLP) model that combines the strengths of two different approaches:\n",
      "\n",
      "1. **Retrieval-based models**: These models focus on finding relevant information from a large database or knowledge base to answer a question or generate text. They typically work by searching for similar or related texts and then using that information to generate a response.\n",
      "2. **Generative models**: These models, on other hand, generate text from scratch, often using complex algorithms and neural networks to produce coherent and context-specific text.\n",
      "\n",
      "The RAG model **Retrieval-Augmented Generator** tries to get the best of both worlds by integrating these two approaches:\n",
      "\n",
      "**How RAG works:**\n",
      "\n",
      "1. **Retrieval step**: Given a prompt or question, the model searches a large database or knowledge base to retrieve relevant information. This information can be in the form of text passages, documents, or even just a set of keywords.\n",
      "2. **Generator step**: The retrieved information is then fed into a generative model, which uses this information to generate a response. The generator model can be a transformer-based model, such as a sequence-to-sequence model.\n",
      "\n",
      "**Benefits of RAG**:\n",
      "\n",
      "1. **Improved accuracy**: By retrieving relevant information, RAG can produce more accurate and informed responses, especially on complex or open-ended questions.\n",
      "2. **Increased efficiency**: RAG can reduce the need for extensive training data, as it can leverage existing knowledge bases and databases.\n",
      "3. **Flexibility**: RAG can handle a wide range of tasks, such as question-answering, text summarization, and text generation.\n",
      "\n",
      "**Applications**:\n",
      "\n",
      "RAG has been applied to various NLP tasks, including:\n",
      "\n",
      "1. **Question answering**: RAG can be used to answer complex questions by retrieving relevant information from a knowledge base.\n",
      "2. **Text summarization**: RAG can summarize long documents by retrieving key passages and then generating a concise summary.\n",
      "3. **Chatbots and conversational AI**: RAG can power chatbots and conversational AI systems, allowing them to respond to user queries with more accuracy and context.\n",
      "\n",
      "Overall, RAG represents a promising approach to NLP, combining the strengths of retrieval-based and generative models to produce more accurate, informative, and engaging text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"Chromium\";v=\"136\", \"Google Chrome\";v=\"136\", \"Not.A/Brand']\n",
      "Bad pipe message: %s [b'v=\"99\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nsec', b'h-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-R', b'uests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Geck', b' Chrome/136.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,imag', b'webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fetch-Site: none\\r\\nSec-Fetch-Mo', b': navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Dest: document\\r\\nAccept-Encoding: gzip, deflate, br, zstd\\r', b'ccept-Lan', b'age: en-US,en;q=0.9,ar;q=0.8\\r\\nCookie: _xsrf=2|0f98a9cf|e2f5aa73f5a7f4d9c6cb95c6a26383ea|1751986549; us', b'name-127-0-0-1-8888=\"2|1:0|10:1751986567|23:username-127-0-0-1-8888|196:eyJ1c2VybmFtZSI6ICJjYzgxODIy']\n",
      "Bad pipe message: %s [b'liMjU0ZDBmYjg4M2I1N2VkNjQwMDc4MiIsICJuYW1lIjogIkFub255bW91cyBUaHlvbmUiLCAiZGlzcGxheV9uYW']\n",
      "Bad pipe message: %s [b'IjogIkFub255bW91cyBUaHlvbmUiLCAiaW5pdGlhbHMiOiAi']\n",
      "Bad pipe message: %s [b'QiLCAiY29sb3IiOiBudWxsfQ==|bab1b051c114f6626a096327443a3869674ec9aeb02d2d59cd6ba']\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc85e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
